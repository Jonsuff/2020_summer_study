---
layout: post
title:  "GeoNet"
date:   2020-10-15 22:16:13
categories: VODE

---



# GeoNet

| 제목 | GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose |
| ---- | :----------------------------------------------------------: |
| 저자 |                  Zhichao Yin, Jianping Shi                   |
| 출판 |                          CVPR 2018                           |

> Jianping Shi : Pyramid scene parsing network, Libra r-cnn, Path aggregation network ...

### 연구 목적 및 연구 내용 요약

- 앞서 소개한 논문들과 같이 Dense prediction, Visual odometry를 추정하고, 추가적으로 Optical flow를 추정하는 모델이다. 

- 논문의 저자는 Nature of 3D geometry 환경에서 VODE에 접근하였다. 실제로 자연적인 환경에서는 카메라 입장에서 움직임이 변하지 않는 **배경**과 **움직이는 물체**가 존재한다. 논문의 주된 목적은 배경과 움직이는 물체에 대해 각각 다른 학습방식을 사용하겠다는 것이다.

  > 배경 : Rigid Flow
  >
  > 물체 : ResFlowNet

- system overview : 

  ![](.\geonet\overview.png)





### 관련 연구 및 내용

- **3D Geometry**

  카메라를 통해 얻어낸 이미지는 실제 3차원 세상을 2차원으로 projection한 결과이다. 따라서 이미지에 찍힌 물체들 사이의 관계는 단순한 픽셀차이로는 알아낼 수 없다.

  이미지를 통해 실제 물체의 정보를 얻어내려면 아래와 같은 방법을 사용한다.

  - Camera calibration : 

    카메라마다 카메라와 특정한 거리에 떨어지는 물체를 이미지로 찍었을 때 한 픽셀당 실제 길이가 정해져있다. 그것을 camera intrinsic(K) 이라고 부르고, 이 값은 대부분 카메라 스펙을 통해 알 수 있다(최근에는 이것조차 추정하는 딥러닝 모델이 연구되었다).

  - Depth : 

    카메라와 물체가 얼마나 떨어져있는가에 대한 수치이다. Camera calibration에서 사용된 K는 단위 depth에 따라 normalize된 좌표에 곱해져 실제 좌표를 얻어내는데 사용된다. 즉 이미지로부터 실제 정보를 얻어내기 위해서는 실제로 그 물체와의 거리인 depth를 K와 함께 사용해야 한다.
    $$
    \\
    K \begin{bmatrix}u \\v \\1\end{bmatrix} = \begin{bmatrix}x \\y \\1\end{bmatrix}\\
    $$
    위와 같이 z(depth)가 1일때 normalize된 좌표가 (u, v, 1)일때 K를 곱하면 (x, y, z)가 나오게 된다. 이들의 역관계를 살펴보면, 
    $$
    \\
    \begin{bmatrix}u \\v \\1\end{bmatrix} = K^{-1}\begin{bmatrix}x \\y \\1\end{bmatrix}\\
    $$
    이와 같고, 만약 z(depth)값이 1이 아닌 실제 값이라면 다음과 같은 결과를 낼 수 있다.
    $$
    \\
    \begin{bmatrix}X \\Y \\Z\end{bmatrix}=Z\begin{bmatrix}u \\v \\1\end{bmatrix}=DK^{-1}\begin{bmatrix}x \\y \\1\end{bmatrix}\\
    $$



### GeoNet

위에서 언급한것과 같이 GeoNet은 camera motion에 대한 Rgid Motion($$f^{rig}_{t \to s}$$), 움직이는 물체에 대한 Residual Motion($$f^{res}_{t \to s}$$) 두 가지로 구성되어 있다. 

![](.\geonet\overview.png)

- $$f^{rig}_{t \to s}$$ : 

  앞선 VODE 논문들에서와 같이 camera intrinsics(K)와 depth를 통해 타겟 이미지의 점 $$p_t$$를 소스 이미지로 변환한 $$p_s$$와 실제 $$p_t$$의 차이를 이용하여 소스 이미지와 타겟 이미지 사이의 camera flow를 구해낼 수 있다. 

  ![](.\geonet\rigid.png)

  ![](.\geonet\ttos.png)

  위의 식에서 $$K^{-1}p_t$$는 타겟 이미지에서 normalize된 좌표이고, 여기에 $$D_t(p_t)$$가 곱해져 실제 월드의 좌표로 변환한 뒤 $$T_{t \to s}$$를 사용하여 소스 이미지에서 normalize된 좌표를 만들고 마지막으로 $$K$$를 곱해주어 소스 이미지에서의 좌표를 구해낸다. 즉 소스 이미지에서의 좌표는 다음 식과 같다.
  $$
  \\p_s = KT_{t \to s} D_t(p_t)K^{-1}p_t\\
  $$
  $$f^{rig}_{t \to s}$$에 대한 식을 다시 살펴보면, 
  $$
  \\ f^{rig}_{t \to s} = p_s - p_t\\
  $$
  인데, 이는 소스이미지에서 본 점과 타겟이미지에서 본 점의 차이를 구함으로써 이들 둘 사이의 flow를 얻어내는 것이다.

  

  

- $$f^{res}_{t \to s}$$ : 

  ResFlowNet을 사용하여 움직이는 물체에 대한 flow를 얻어낸다. overview의 그림에서는 단순히 Rigid Flow만 입력되는것처럼 보이지만, 실제 코드에서는 Input frame까지 같이 입력된다고 한다.

  ![](.\geonet\res.png)

  논문 깃헙에 올라간 코드를 분석하면, Input frame과 Rgid flow가 channel로 합쳐저서 입력이 되고, residual block과 skip connection이 추가된 후 4가지 스케일에 대해 출력한다.

- $$f^{full}_{t\to s}$$ : 

  전체적인 flow는 앞선 두 flow를 더한것과 같다.
  $$
  f^{full}_{t \to s} = f^{rig}_{t \to s} + f^{res}_{t \to s}
  $$
  

### Loss functions

GeoNet에서 사용한 loss function 종류는 다음과 같다.

- Rigid warping loss : 

  Rigid flow를 이용하여 연산
  $$
  \\
  L_{rw} = \alpha {1-SSIM(I_t, \tilde{I}^{rig}_s) \over 2}+(1-\alpha) \rVert I_t - \tilde{I}^{rig}_s \rVert_1
  \\
  $$

  > $$\tilde{I^{rig}_s}$$ : 소스 이미지를 타겟 이미지 시점으로 변환한 것의 inverse
  >
  > $$\alpha$$ : 0.85

- Edge-Aware Depth Smoothness Loss : 

  Optical flow에서 사용되는 loss로, total variation loss라고도 부른다. 기본적인 컨셉은 "근처에 있는 픽셀끼리는 값이 비슷할 것이다"를 이용한 것이다. 즉 서로 근처에 있는 픽셀끼리의 depth값은 급격히 변하지 않게끔 만들어주는 loss이다.
  $$
  \\L_{ds} = \sum_{p_t} |\triangledown D(p_t)| \cdot (e^{- |\triangledown I(p_t)|})^T\\
  $$

  > $$\triangledown$$ : 벡터 미분연산
  >
  > $$|\triangledown D(p_t)|$$ : depth가 급격하게 변하지 않게 함
  >
  > $$(e^{- |\triangledown I(p_t)|})^T$$ : 경계면에서는 depth변화를 크게함

- Flow warping loss : 

  Full flow를 이용하여 연산
  $$
  \\
  L_{fw} = \alpha {1-SSIM(I_t, \tilde{I}^{full}_s) \over 2} + (1-\alpha) \rVert I_t - \tilde{I}^{full}_s \rVert_1
  \\
  $$

- Edge-Aware Flow Smoothness Loss : 

  Edge-Aware Depth Smoothness Loss와 비슷한 용도로 쓰이며, 급격한 변화를 막는 대상이 depth가 아닌 flow 이다.
  $$
  \\L_{ds} = \sum_{p_t} |\triangledown f^{full}_{t \to s}(p_t)| \cdot (e^{- |\triangledown I(p_t)|})^T\\
  $$

- Geometric Consistency Loss : 

  Optical flow를 구할 때 Occlusion reasoning을 위해 많이 사용한다.

  ![](.\geonet\gcl.png)

  위와 같은 상황에서는 이론적으로 $$H^f_{s_p}$$를 통해 변화된 p'를 다시 $$H^b_{s_p}$$를 통해 변화시키면 그 값은 동일해야 한다. 이를 수식으로 표현하면 다음과 같은데,
  $$
  \\
  \Delta f^{full}_{t \to s}(p_t)\\
  $$
  Full flow에서 타겟의 픽셀을 소스의 픽셀로 변화시킨것(forward)와, 그 역방향(backward)의 차이에 대한 변화율이라고 생각할 수 있다.

  결과적으로 Geometric Consistency Loss식은 다음과 같다.
  $$
  \\
  L_{gc} = \sum_{p_t}[\delta (p_t)] \cdot \rVert \Delta f^{full}_{t \to s}(p_t) \rVert_1
  \\
  $$
  여기서 $$[\delta (p_t)]$$는 다음 명제가 참일때 1, 거짓일때 0이 된다.
  $$
  \\
  \rVert \Delta f^{full}_{t \to s}(p_t) \rVert_2 < max\{ \alpha, \beta \rVert f^{full}_{t \to s}(p_t) \rVert_2 \}
  \\
  $$

  > $$\alpha$$ : 3.0
  >
  > $$\beta$$ : 0.05



결과적으로 최종 Loss는 위의 모든 Loss들을 더한것과 같다.
$$
\\
L = \sum_l \sum_{<t, s>} \{L_{rw} + \lambda_{ds}L_{ds} + L_{fw} + \lambda_{fs} L_{fs} + \lambda_{gc} L_{gc} \}\\
$$


### Result

- Depth estimation : 

  다른 모델들과 비교한 Depth estimation 결과이다.

  ![](.\geonet\depthest.png)

  수치상으로 비교한 결과 이다.

  ![](.\geonet\graph.png)

- Optical Flow estimation : 

  gt, supervised 모델과 비교한 flow estimation 결과이다.

  ![](.\geonet\vssupervised.png)

  

  수치상으로 비교한 결과 이다.

  ![](.\geonet\optical.png)

- Camera pose estimation : 

  다른 모델과 비교한 camera pose estimation 결과이다.

  ![](.\geonet\pose.png)



### 결론

Depth, Camera pose, Optical flow 와 같이 총 세 가지의 추정을 통해 unsupervised learning을 진행하는 GeoNet은 다른 supervised learning을 진행하는 모델보다는 성능이 떨어지지만, unsupervised 모델들 중에서는 준수한 성능을 보인다.